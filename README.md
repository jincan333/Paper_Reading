# Safety Alignment Paper Reading

This repository is dedicated to tracking and summarizing papers related to Safety Alignment in Large Language Models (LLMs). It aims to collect and organize the latest research progress, core questions, and innovative methods in this field.

## Paper List

| Time | Paper | Research Question/Idea | Method |
| :--- | :--- | :--- | :--- |
| 2025-05 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. | Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. |
